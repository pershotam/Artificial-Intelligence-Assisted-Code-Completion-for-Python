{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Github](https://img.shields.io/github/stars/lab-ml/python_autocomplete?style=social)](https://github.com/lab-ml/python_autocomplete)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lab-ml/python_autocomplete/blob/master/notebooks/evaluate.ipynb)\n",
    "\n",
    "# Evaluate a model trained on predicting Python code\n",
    "\n",
    "This notebook evaluates a model trained on Python code.\n",
    "\n",
    "Here's a link to [training notebook](https://github.com/lab-ml/python_autocomplete/blob/master/notebooks/train.ipynb)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lab-ml/python_autocomplete/blob/master/notebooks/train.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: labml in /home/varuna/ml/lab-ml/labml (0.4.54)\n",
      "Requirement already satisfied: gitpython in /home/varuna/miniconda/envs/torch/lib/python3.8/site-packages (from labml) (3.1.11)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/varuna/miniconda/envs/torch/lib/python3.8/site-packages (from labml) (5.3.1)\n",
      "Requirement already satisfied: numpy in /home/varuna/miniconda/envs/torch/lib/python3.8/site-packages (from labml) (1.19.2)\n",
      "Collecting labml_python_autocomplete\n",
      "  Downloading labml_python_autocomplete-0.0.5-py3-none-any.whl (14 kB)\n",
      "Collecting labml\n",
      "  Downloading labml-0.4.101-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 445 kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting labml-helpers>=0.4.70\n",
      "  Downloading labml_helpers-0.4.74-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: torch in /home/varuna/miniconda/envs/torch/lib/python3.8/site-packages (from labml-helpers>=0.4.70->labml_python_autocomplete) (1.7.1)\n",
      "Collecting labml-nn>=0.4.70torch\n",
      "  Downloading labml_nn-0.4.86-py3-none-any.whl (145 kB)\n",
      "\u001b[K     |████████████████████████████████| 145 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting einops\n",
      "  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/varuna/miniconda/envs/torch/lib/python3.8/site-packages (from gitpython->labml) (4.0.5)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /home/varuna/miniconda/envs/torch/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython->labml) (3.0.4)\n",
      "Requirement already satisfied: typing_extensions in /home/varuna/miniconda/envs/torch/lib/python3.8/site-packages (from torch->labml-helpers>=0.4.70->labml_python_autocomplete) (3.7.4.3)\n",
      "Installing collected packages: labml, labml-helpers, einops, labml-nn, labml-python-autocomplete\n",
      "  Attempting uninstall: labml\n",
      "    Found existing installation: labml 0.4.54\n",
      "    Uninstalling labml-0.4.54:\n",
      "      Successfully uninstalled labml-0.4.54\n",
      "Successfully installed einops-0.3.0 labml-0.4.101 labml-helpers-0.4.74 labml-nn-0.4.86 labml-python-autocomplete-0.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install labml labml_python_autocomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from labml import experiment, logger, lab\n",
    "from labml_helpers.module import Module\n",
    "from labml.analytics import ModelProbe\n",
    "from labml.logger import Text, Style, inspect\n",
    "from labml.utils.pytorch import get_modules\n",
    "from labml.utils.cache import cache\n",
    "from labml_helpers.datasets.text import TextDataset\n",
    "\n",
    "from python_autocomplete.train import Configs\n",
    "from python_autocomplete.evaluate import Predictor\n",
    "from python_autocomplete.evaluate.beam_search import NextWordPredictionComplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model from a training run. For this demo I'm loading from a run I trained at home.\n",
    "\n",
    "[![View Run](https://img.shields.io/badge/labml-experiment-brightgreen)](https://web.lab-ml.com/run?uuid=39b03a1e454011ebbaff2b26e3148b3d)\n",
    "\n",
    "If you have a locally trained model load it directly with:\n",
    "\n",
    "```python\n",
    "run_uuid = 'RUN_UUID'\n",
    "checkpoint = None # Get latest checkpoint\n",
    "```\n",
    "\n",
    "`load_bundle` will download an archive with a saved checkpoint (pretrained model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\"><span style=\"color: #208FFB\">Run a6cff3706ec411ebadd9bf753b33bae6 exists</span>\n",
       "<span style=\"color: #208FFB\">Checkpoint 702925824 exists</span>\n",
       "Extract bundle<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t1,534.94ms</span>\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run_uuid = 'a6cff3706ec411ebadd9bf753b33bae6'\n",
    "# checkpoint = None\n",
    "\n",
    "run_uuid, checkpoint = experiment.load_bundle(\n",
    "    lab.get_path() / 'saved_checkpoint.tar.gz',\n",
    "    url='https://github.com/lab-ml/python_autocomplete/releases/download/0.0.5/bundle.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize `Configs` object defined in [`train.py`](https://github.com/lab-ml/python_autocomplete/blob/master/python_autocomplete/train.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new experiment in evaluation mode. In evaluation mode a new training run is not created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load custom configurations/hyper-parameters used in the training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 32,\n",
       " 'is_token_by_token': True,\n",
       " 'mem_len': 256,\n",
       " 'model': 'transformer_xl_model',\n",
       " 'n_layers': 6,\n",
       " 'optimizer.learning_rate': 0.000125,\n",
       " 'optimizer.optimizer': 'AdamW',\n",
       " 'state_updater': 'transformer_memory',\n",
       " 'text.batch_size': 12,\n",
       " 'text.is_shuffle': False,\n",
       " 'text.seq_len': 256,\n",
       " 'text.tokenizer': 'bpe'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_conf = experiment.load_configs(run_uuid)\n",
    "custom_conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the custom configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_conf['device.use_cuda'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\"></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment.configs(conf, custom_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set models for saving and loading. This will load `conf.model` from the specified run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\">Prepare model...\n",
       "  Prepare n_tokens...\n",
       "    Prepare tokenizer<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t4.84ms</span>\n",
       "  Prepare n_tokens<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t10.12ms</span>\n",
       "  Prepare transformer<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t2.95ms</span>\n",
       "  Prepare ffn<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t1.67ms</span>\n",
       "  Prepare device...\n",
       "    Prepare device_info<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t1.84ms</span>\n",
       "  Prepare device<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t4.15ms</span>\n",
       "Prepare model<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t147.69ms</span>\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment.add_pytorch_models({'model': conf.model})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify which run to load from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.load(run_uuid, checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\">Selected <span style=\"color: #60C6C8\">experiment</span> = <strong>source_code</strong> <span style=\"color: #60C6C8\">run</span> = <strong>a6cff3706ec411ebadd9bf753b33bae6</strong> <span style=\"color: #60C6C8\">checkpoint</span> = <strong>702,925,824</strong>\n",
       "Loading checkpoint<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t389.90ms</span>\n",
       "\n",
       "<strong><span style=\"text-decoration: underline\">Notebook Experiment</span></strong>: <span style=\"color: #208FFB\">07e6549a74ed11ec97e2acde48001122</span>\n",
       "\t[dirty]: <strong><span style=\"color: #DDB62B\">\"serve without debug\"</span></strong>\n",
       "\tloaded from: <span style=\"color: #D160C4\">a6cff3706ec411ebadd9bf753b33bae6</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<labml.internal.experiment.watcher.ExperimentWatcher at 0x7f8eaf318310>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the `Predictor` defined in [`evaluate.py`](https://github.com/lab-ml/python_autocomplete/blob/master/python_autocomplete/evaluate.py).\n",
    "\n",
    "We load `stoi` and `itos` from cache, so that we don't have to read the dataset to generate them. `stoi` is the map for character to an integer index and `itos` is the map of integer to character map. These indexes are used in the model embeddings for each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\">Prepare state_updater<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t4.30ms</span>\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = Predictor(conf.model, conf.text.tokenizer,\n",
    "              state_updater=conf.state_updater,\n",
    "              is_token_by_token=conf.is_token_by_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = conf.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup probing to extract attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = ModelProbe(conf.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A python prompt to test completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"from torch import nn\n",
    "\n",
    "from labml_helpers.module import Module\n",
    "from labml_nn.lstm import LSTM\n",
    "\n",
    "\n",
    "class LSTM(Module):\n",
    "    def __init__(self, *,\n",
    "                 n_tokens: int,\n",
    "                 embedding_size: int,\n",
    "                 hidden_size int,\n",
    "                 n_layers int):\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a token. `get_token` predicts character by character greedily (no beam search) until it find and end of token character (non alpha-numeric character)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped, prompt = p.rstrip(PROMPT)\n",
    "rest = PROMPT[len(stripped):]\n",
    "prediction_complete = NextWordPredictionComplete(rest, 5)\n",
    "prompt = torch.tensor(prompt, dtype=torch.long).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 193 ms, sys: 47.4 ms, total: 241 ms\n",
      "Wall time: 218 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.07585359086023313, 'super'),\n",
       " (0.0010850478390376612, '\"\"\"\\n        '),\n",
       " (0.0007989550358615816, '        ')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "predictions = p.get_next_word(prompt, None, rest, [1.], prediction_complete, 5)\n",
    "predictions.sort(key=lambda x: -x[0])\n",
    "[(pred.prob, pred.text[len(rest):]) for pred in predictions]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
