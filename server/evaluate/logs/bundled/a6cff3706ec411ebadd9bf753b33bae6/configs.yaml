accuracy:
  computed: labml_helpers.metrics.accuracy.Accuracy
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: accuracy
  options: []
  order: 9
  type: <class 'labml_helpers.metrics.accuracy.Accuracy'>
  value: null
d_model:
  computed: 512
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: d_model
  options: []
  order: 3
  type: <class 'int'>
  value: null
device:
  computed: cuda:0
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: device
  options: []
  order: 7
  type: <class 'torch.device'>
  value: null
device.cuda_device:
  computed: 0
  is_explicitly_specified: true
  is_hyperparam: false
  is_meta: null
  name: cuda_device
  options: []
  order: 3
  type: <class 'int'>
  value: 0
device.device:
  computed: cuda:0
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: device
  options:
  - _device
  order: 0
  type: <class 'torch.device'>
  value: null
device.device_info:
  computed: GPU:0 - GeForce RTX 2080 Ti
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: device_info
  options:
  - _device_info
  order: 1
  type: <class 'labml_helpers.device.DeviceInfo'>
  value: null
device.use_cuda:
  computed: true
  is_explicitly_specified: false
  is_hyperparam: false
  is_meta: null
  name: use_cuda
  options: []
  order: 2
  type: <class 'bool'>
  value: null
dropout:
  computed: 0.2
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: dropout
  options: []
  order: 6
  type: <class 'float'>
  value: null
epochs:
  computed: 32
  is_explicitly_specified: true
  is_hyperparam: null
  is_meta: null
  name: epochs
  options: []
  order: 19
  type: <class 'int'>
  value: 32
grad_norm_clip:
  computed: 1.0
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: grad_norm_clip
  options: []
  order: 30
  type: <class 'float'>
  value: null
inner_iterations:
  computed: 100
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: inner_iterations
  options: []
  order: 13
  type: <class 'int'>
  value: null
is_loop_on_interrupt:
  computed: false
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: true
  name: is_loop_on_interrupt
  options: []
  order: 25
  type: <class 'bool'>
  value: null
is_save_models:
  computed: true
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: true
  name: is_save_models
  options: []
  order: 21
  type: <class 'bool'>
  value: null
is_token_by_token:
  computed: true
  is_explicitly_specified: true
  is_hyperparam: null
  is_meta: null
  name: is_token_by_token
  options: []
  order: 26
  type: <class 'bool'>
  value: true
log_new_line_interval:
  computed: 1
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: true
  name: log_new_line_interval
  options: []
  order: 22
  type: <class 'int'>
  value: null
log_write_interval:
  computed: 1
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: true
  name: log_write_interval
  options: []
  order: 23
  type: <class 'int'>
  value: null
loop_count:
  computed: 32
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: true
  name: loop_count
  options:
  - _data_loop_count
  order: 18
  type: <class 'int'>
  value: null
loop_step:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: true
  name: loop_step
  options: []
  order: 20
  type: <class 'int'>
  value: null
loss_func:
  computed: "CrossEntropyLoss(\n  (loss): CrossEntropyLoss()\n)"
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: loss_func
  options:
  - _loss_func
  order: 29
  type: CrossEntropyLoss
  value: null
mem_len:
  computed: 256
  is_explicitly_specified: true
  is_hyperparam: null
  is_meta: null
  name: mem_len
  options: []
  order: 28
  type: <class 'int'>
  value: 256
mode:
  computed: labml_helpers.train_valid.ModeState
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: mode
  options:
  - from_type
  order: 8
  type: <class 'labml_helpers.train_valid.ModeState'>
  value: null
model:
  computed: "TransformerXLModel(\n  (src_embed): Embedding(1097, 512)\n  (transformer):\
    \ TransformerXL(\n    (layers): ModuleList(\n      (0): TransformerXLLayer(\n\
    \        (self_attn): RelativeMultiHeadAttention(\n          (query): PrepareForMultiHeadAttention(\n\
    \            (linear): Linear(in_features=512, out_features=512, bias=False)\n\
    \          )\n          (key): PrepareForMultiHeadAttention(\n            (linear):\
    \ Linear(in_features=512, out_features=512, bias=False)\n          )\n       \
    \   (value): PrepareForMultiHeadAttention(\n            (linear): Linear(in_features=512,\
    \ out_features=512, bias=True)\n          )\n          (softmax): Softmax(dim=1)\n\
    \          (output): Linear(in_features=512, out_features=512, bias=True)\n  \
    \        (dropout): Dropout(p=0.2, inplace=False)\n        )\n        (feed_forward):\
    \ FeedForward(\n          (layer1): Linear(in_features=512, out_features=2048,\
    \ bias=True)\n          (layer2): Linear(in_features=2048, out_features=512, bias=True)\n\
    \          (dropout): Dropout(p=0.2, inplace=False)\n          (activation): ReLU()\n\
    \        )\n        (dropout): Dropout(p=0.2, inplace=False)\n        (norm_self_attn):\
    \ LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm_ff): LayerNorm((512,),\
    \ eps=1e-05, elementwise_affine=True)\n      )\n      (1): TransformerXLLayer(\n\
    \        (self_attn): RelativeMultiHeadAttention(\n          (query): PrepareForMultiHeadAttention(\n\
    \            (linear): Linear(in_features=512, out_features=512, bias=False)\n\
    \          )\n          (key): PrepareForMultiHeadAttention(\n            (linear):\
    \ Linear(in_features=512, out_features=512, bias=False)\n          )\n       \
    \   (value): PrepareForMultiHeadAttention(\n            (linear): Linear(in_features=512,\
    \ out_features=512, bias=True)\n          )\n          (softmax): Softmax(dim=1)\n\
    \          (output): Linear(in_features=512, out_features=512, bias=True)\n  \
    \        (dropout): Dropout(p=0.2, inplace=False)\n        )\n        (feed_forward):\
    \ FeedForward(\n          (layer1): Linear(in_features=512, out_features=2048,\
    \ bias=True)\n          (layer2): Linear(in_features=2048, out_features=512, bias=True)\n\
    \          (dropout): Dropout(p=0.2, inplace=False)\n          (activation): ReLU()\n\
    \        )\n        (dropout): Dropout(p=0.2, inplace=False)\n        (norm_self_attn):\
    \ LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm_ff): LayerNorm((512,),\
    \ eps=1e-05, elementwise_affine=True)\n      )\n      (2): TransformerXLLayer(\n\
    \        (self_attn): RelativeMultiHeadAttention(\n          (query): PrepareForMultiHeadAttention(\n\
    \            (linear): Linear(in_features=512, out_features=512, bias=False)\n\
    \          )\n          (key): PrepareForMultiHeadAttention(\n            (linear):\
    \ Linear(in_features=512, out_features=512, bias=False)\n          )\n       \
    \   (value): PrepareForMultiHeadAttention(\n            (linear): Linear(in_features=512,\
    \ out_features=512, bias=True)\n          )\n          (softmax): Softmax(dim=1)\n\
    \          (output): Linear(in_features=512, out_features=512, bias=True)\n  \
    \        (dropout): Dropout(p=0.2, inplace=False)\n        )\n        (feed_forward):\
    \ FeedForward(\n          (layer1): Linear(in_features=512, out_features=2048,\
    \ bias=True)\n          (layer2): Linear(in_features=2048, out_features=512, bias=True)\n\
    \          (dropout): Dropout(p=0.2, inplace=False)\n          (activation): ReLU()\n\
    \        )\n        (dropout): Dropout(p=0.2, inplace=False)\n        (norm_self_attn):\
    \ LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm_ff): LayerNorm((512,),\
    \ eps=1e-05, elementwise_affine=True)\n      )\n      (3): TransformerXLLayer(\n\
    \        (self_attn): RelativeMultiHeadAttention(\n          (query): PrepareForMultiHeadAttention(\n\
    \            (linear): Linear(in_features=512, out_features=512, bias=False)\n\
    \          )\n          (key): PrepareForMultiHeadAttention(\n            (linear):\
    \ Linear(in_features=512, out_features=512, bias=False)\n          )\n       \
    \   (value): PrepareForMultiHeadAttention(\n            (linear): Linear(in_features=512,\
    \ out_features=512, bias=True)\n          )\n          (softmax): Softmax(dim=1)\n\
    \          (output): Linear(in_features=512, out_features=512, bias=True)\n  \
    \        (dropout): Dropout(p=0.2, inplace=False)\n        )\n        (feed_forward):\
    \ FeedForward(\n          (layer1): Linear(in_features=512, out_features=2048,\
    \ bias=True)\n          (layer2): Linear(in_features=2048, out_features=512, bias=True)\n\
    \          (dropout): Dropout(p=0.2, inplace=False)\n          (activation): ReLU()\n\
    \        )\n        (dropout): Dropout(p=0.2, inplace=False)\n        (norm_self_attn):\
    \ LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm_ff): LayerNorm((512,),\
    \ eps=1e-05, elementwise_affine=True)\n      )\n      (4): TransformerXLLayer(\n\
    \        (self_attn): RelativeMultiHeadAttention(\n          (query): PrepareForMultiHeadAttention(\n\
    \            (linear): Linear(in_features=512, out_features=512, bias=False)\n\
    \          )\n          (key): PrepareForMultiHeadAttention(\n            (linear):\
    \ Linear(in_features=5 [[...]]"
  is_explicitly_specified: true
  is_hyperparam: null
  is_meta: null
  name: model
  options:
  - lstm_model
  - rhn_model
  - transformer_model
  - transformer_xl_model
  order: 0
  type: <class 'labml_helpers.module.Module'>
  value: transformer_xl_model
n_layers:
  computed: 6
  is_explicitly_specified: true
  is_hyperparam: null
  is_meta: null
  name: n_layers
  options: []
  order: 5
  type: <class 'int'>
  value: 6
n_tokens:
  computed: 1097
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: n_tokens
  options:
  - _n_tokens
  order: 1
  type: <class 'int'>
  value: null
optimizer:
  computed: "AdamWarmup (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9,\
    \ 0.999)\n    eps: 1e-08\n    lr: 0.000125\n    warmup: 2000\n    weight_decay:\
    \ 0.0\n)"
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: optimizer
  options:
  - _optimizer
  order: 31
  type: <class 'torch.optim.adam.Adam'>
  value: null
optimizer.amsgrad:
  computed: false
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: amsgrad
  options: []
  order: 9
  type: <class 'bool'>
  value: null
optimizer.betas:
  computed:
  - 0.9
  - 0.999
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: betas
  options: []
  order: 3
  type: typing.Tuple[float, float]
  value: null
optimizer.d_model:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: d_model
  options: []
  order: -1
  type: <class 'int'>
  value: 512
optimizer.degenerate_to_sgd:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: degenerate_to_sgd
  options: []
  order: -1
  type: <class 'bool'>
  value: null
optimizer.eps:
  computed: 1.0e-08
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: eps
  options: []
  order: 4
  type: <class 'float'>
  value: null
optimizer.learning_rate:
  computed: 0.000125
  is_explicitly_specified: true
  is_hyperparam: null
  is_meta: null
  name: learning_rate
  options: []
  order: 2
  type: <class 'float'>
  value: 0.000125
optimizer.momentum:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: momentum
  options: []
  order: -1
  type: <class 'float'>
  value: null
optimizer.optimized_adam_update:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: optimized_adam_update
  options: []
  order: -1
  type: <class 'bool'>
  value: null
optimizer.optimizer:
  computed: "AdamWarmup (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9,\
    \ 0.999)\n    eps: 1e-08\n    lr: 0.000125\n    warmup: 2000\n    weight_decay:\
    \ 0.0\n)"
  is_explicitly_specified: true
  is_hyperparam: null
  is_meta: null
  name: optimizer
  options:
  - SGD
  - Adam
  - AdamW
  - RAdam
  - AdaBelief
  - Noam
  - AdamWarmupCosineDecay
  order: 0
  type: <class 'torch.optim.adam.Adam'>
  value: AdamW
optimizer.parameters:
  computed: <generator object Module.parameters at 0x7fa2fc91f6d0>
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: true
  name: parameters
  options: []
  order: 1
  type: <built-in function any>
  value: <generator object Module.parameters at 0x7fa2fc91f6d0>
optimizer.rectify:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: rectify
  options: []
  order: -1
  type: <class 'bool'>
  value: null
optimizer.total_steps:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: total_steps
  options: []
  order: -1
  type: <class 'int'>
  value: null
optimizer.warmup:
  computed: 2000
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: warmup
  options: []
  order: 10
  type: <class 'int'>
  value: null
optimizer.weight_decay:
  computed: 0.0
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: weight_decay
  options: []
  order: 6
  type: <class 'float'>
  value: null
optimizer.weight_decay_absolute:
  computed: false
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: weight_decay_absolute
  options: []
  order: 8
  type: <class 'bool'>
  value: null
optimizer.weight_decay_obj:
  computed: labml_nn.optimizers.WeightDecay
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: weight_decay_obj
  options:
  - L2
  order: 5
  type: <class 'labml_nn.optimizers.WeightDecay'>
  value: null
optimizer.weight_decouple:
  computed: true
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: weight_decouple
  options: []
  order: 7
  type: <class 'bool'>
  value: null
rhn_depth:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: rhn_depth
  options: []
  order: -1
  type: <class 'int'>
  value: null
rnn_size:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: rnn_size
  options: []
  order: -1
  type: <class 'int'>
  value: null
save_models_interval:
  computed: 1
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: true
  name: save_models_interval
  options: []
  order: 24
  type: <class 'int'>
  value: null
state:
  computed: labml_helpers.metrics.simple_state.SimpleStateModule
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: state
  options: []
  order: 10
  type: <class 'labml_helpers.metrics.simple_state.SimpleStateModule'>
  value: null
state_modules:
  computed:
  - labml_helpers.metrics.accuracy.Accuracy
  - labml_helpers.metrics.simple_state.SimpleStateModule
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: state_modules
  options: []
  order: 14
  type: typing.List[labml_helpers.metrics.StateModule]
  value:
  - labml_helpers.metrics.accuracy.Accuracy
  - labml_helpers.metrics.simple_state.SimpleStateModule
state_updater:
  computed: MemoryUpdater
  is_explicitly_specified: true
  is_hyperparam: null
  is_meta: null
  name: state_updater
  options:
  - simple
  - transformer_memory
  order: 27
  type: StateUpdater
  value: transformer_memory
text:
  computed: python_autocomplete.dataset.dataset.SourceCodeDataConfigs
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: text
  options: []
  order: 2
  type: <class 'python_autocomplete.dataset.dataset.SourceCodeDataConfigs'>
  value: null
text.batch_size:
  computed: 12
  is_explicitly_specified: true
  is_hyperparam: null
  is_meta: null
  name: batch_size
  options: []
  order: 5
  type: <class 'int'>
  value: 12
text.dataset:
  computed: 151.35M, 16.27
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: dataset
  options:
  - default
  order: 2
  type: <class 'python_autocomplete.dataset.dataset.SourceCodeDataset'>
  value: null
text.is_load_data:
  computed: true
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: is_load_data
  options: []
  order: 3
  type: <class 'bool'>
  value: null
text.is_shuffle:
  computed: false
  is_explicitly_specified: true
  is_hyperparam: null
  is_meta: null
  name: is_shuffle
  options: []
  order: 7
  type: <class 'bool'>
  value: false
text.retrain_tokenizer:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: retrain_tokenizer
  options: []
  order: -1
  type: <class 'bool'>
  value: null
text.seq_len:
  computed: 256
  is_explicitly_specified: true
  is_hyperparam: null
  is_meta: null
  name: seq_len
  options: []
  order: 6
  type: <class 'int'>
  value: 256
text.tokenizer:
  computed: python_autocomplete.dataset.bpe.BPE
  is_explicitly_specified: true
  is_hyperparam: null
  is_meta: null
  name: tokenizer
  options:
  - bpe
  - char
  order: 0
  type: <class 'python_autocomplete.dataset.Tokenizer'>
  value: bpe
text.train_loader:
  computed: torch.utils.data.dataloader.DataLoader
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: train_loader
  options:
  - _train_loader
  order: 8
  type: <class 'torch.utils.data.dataloader.DataLoader'>
  value: null
text.truncate_data:
  computed: 0
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: truncate_data
  options: []
  order: 4
  type: <class 'int'>
  value: null
text.valid_loader:
  computed: torch.utils.data.dataloader.DataLoader
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: valid_loader
  options:
  - _valid_loader
  order: 1
  type: <class 'torch.utils.data.dataloader.DataLoader'>
  value: null
train_loader:
  computed: torch.utils.data.dataloader.DataLoader
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: train_loader
  options:
  - _train_loader
  order: 16
  type: <class 'torch.utils.data.dataloader.DataLoader'>
  value: null
trainer:
  computed: labml_helpers.train_valid.Trainer
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: trainer
  options:
  - _default_trainer
  order: 15
  type: <class 'labml_helpers.train_valid.Trainer'>
  value: null
training_loop:
  computed: LabTrainingLoop
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: training_loop
  options:
  - _loop_configs
  order: 17
  type: <class 'labml_helpers.training_loop.TrainingLoop'>
  value: null
transformer:
  computed: labml_nn.transformers.configs.TransformerConfigs
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: transformer
  options:
  - default_transformer
  order: 4
  type: <class 'labml_nn.transformers.configs.TransformerConfigs'>
  value: null
transformer.d_model:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: d_model
  options: []
  order: -1
  type: <class 'int'>
  value: 512
transformer.decoder:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: decoder
  options:
  - default
  order: -1
  type: <class 'labml_nn.transformers.models.Decoder'>
  value: null
transformer.decoder_attn:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: decoder_attn
  options:
  - mha
  - relative
  order: -1
  type: <class 'labml_nn.transformers.mha.MultiHeadAttention'>
  value: null
transformer.decoder_layer:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: decoder_layer
  options:
  - default
  order: -1
  type: <class 'labml_nn.transformers.models.TransformerLayer'>
  value: null
transformer.decoder_mem_attn:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: decoder_mem_attn
  options:
  - mha
  - relative
  order: -1
  type: <class 'labml_nn.transformers.mha.MultiHeadAttention'>
  value: null
transformer.dropout:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: dropout
  options: []
  order: -1
  type: <class 'float'>
  value: 0.2
transformer.encoder:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: encoder
  options:
  - default
  order: -1
  type: <class 'labml_nn.transformers.models.Encoder'>
  value: null
transformer.encoder_attn:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: encoder_attn
  options:
  - mha
  - relative
  order: -1
  type: <class 'labml_nn.transformers.mha.MultiHeadAttention'>
  value: null
transformer.encoder_decoder:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: encoder_decoder
  options:
  - default
  order: -1
  type: <class 'labml_nn.transformers.models.EncoderDecoder'>
  value: null
transformer.encoder_layer:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: encoder_layer
  options:
  - default
  order: -1
  type: <class 'labml_nn.transformers.models.TransformerLayer'>
  value: null
transformer.ffn:
  computed: labml_nn.transformers.configs.FeedForwardConfigs
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: ffn
  options:
  - default
  order: 1
  type: <class 'labml_nn.transformers.configs.FeedForwardConfigs'>
  value: null
transformer.ffn.activation:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: activation
  options:
  - ReLU
  - GELU
  order: -1
  type: <class 'torch.nn.modules.module.Module'>
  value: null
transformer.ffn.bias1:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: bias1
  options: []
  order: -1
  type: <class 'bool'>
  value: null
transformer.ffn.bias2:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: bias2
  options: []
  order: -1
  type: <class 'bool'>
  value: null
transformer.ffn.bias_gate:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: bias_gate
  options: []
  order: -1
  type: <class 'bool'>
  value: null
transformer.ffn.d_ff:
  computed: 2048
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: d_ff
  options: []
  order: 0
  type: <class 'int'>
  value: null
transformer.ffn.d_model:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: d_model
  options: []
  order: -1
  type: <class 'int'>
  value: null
transformer.ffn.dropout:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: dropout
  options: []
  order: -1
  type: <class 'float'>
  value: null
transformer.ffn.ffn:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: ffn
  options:
  - default
  order: -1
  type: <class 'labml_nn.transformers.feed_forward.FeedForward'>
  value: null
transformer.ffn.glu_variant:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: glu_variant
  options:
  - GEGLU
  - Bilinear
  - GLU
  - SwiGLU
  - ReGLU
  order: -1
  type: <class 'str'>
  value: null
transformer.ffn.is_gated:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: is_gated
  options: []
  order: -1
  type: <class 'bool'>
  value: null
transformer.generator:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: generator
  options:
  - default
  order: -1
  type: <class 'labml_nn.transformers.models.Generator'>
  value: null
transformer.n_heads:
  computed: 8
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: n_heads
  options: []
  order: 0
  type: <class 'int'>
  value: null
transformer.n_layers:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: n_layers
  options: []
  order: -1
  type: <class 'int'>
  value: 6
transformer.n_src_vocab:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: n_src_vocab
  options: []
  order: -1
  type: <class 'int'>
  value: 1097
transformer.n_tgt_vocab:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: n_tgt_vocab
  options: []
  order: -1
  type: <class 'int'>
  value: 1097
transformer.src_embed:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: src_embed
  options:
  - fixed_pos
  - learned_pos
  - no_pos
  order: -1
  type: <class 'labml_helpers.module.Module'>
  value: null
transformer.tgt_embed:
  computed: null
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: tgt_embed
  options:
  - fixed_pos
  - learned_pos
  - no_pos
  order: -1
  type: <class 'labml_helpers.module.Module'>
  value: null
valid_loader:
  computed: torch.utils.data.dataloader.DataLoader
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: valid_loader
  options:
  - _valid_loader
  order: 12
  type: <class 'torch.utils.data.dataloader.DataLoader'>
  value: null
validator:
  computed: labml_helpers.train_valid.Trainer
  is_explicitly_specified: false
  is_hyperparam: null
  is_meta: null
  name: validator
  options:
  - _default_validator
  order: 11
  type: <class 'labml_helpers.train_valid.Trainer'>
  value: null
